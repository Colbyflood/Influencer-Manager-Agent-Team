---
phase: 03-llm-negotiation-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - uv.lock
  - src/negotiation/llm/__init__.py
  - src/negotiation/llm/client.py
  - src/negotiation/llm/models.py
  - src/negotiation/llm/prompts.py
  - src/negotiation/llm/knowledge_base.py
  - knowledge_base/general.md
  - knowledge_base/instagram.md
  - knowledge_base/tiktok.md
  - knowledge_base/youtube.md
  - tests/llm/__init__.py
  - tests/llm/test_knowledge_base.py
autonomous: true
requirements:
  - KB-01
  - KB-02
  - KB-03

user_setup:
  - service: anthropic
    why: "LLM API for intent classification and email composition"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/settings/keys)"

must_haves:
  truths:
    - "Knowledge base markdown files exist with per-platform negotiation guidance (Instagram, TikTok, YouTube) and a general playbook"
    - "Non-technical team members can edit knowledge base files without code changes -- files are plain Markdown in a top-level directory"
    - "Knowledge base loader returns combined general + platform-specific content ready for system prompt injection"
    - "LLM Pydantic models define the structured I/O contracts for intent classification and email composition"
    - "Anthropic client wrapper provides configured clients with correct model selection (Haiku for intent, Sonnet for composition)"
  artifacts:
    - path: "knowledge_base/general.md"
      provides: "Cross-platform negotiation playbook (tone rules, forbidden phrases, stage guidance)"
      contains: "Do NOT Say"
    - path: "knowledge_base/instagram.md"
      provides: "Instagram-specific negotiation guidance and example emails"
      contains: "Instagram"
    - path: "knowledge_base/tiktok.md"
      provides: "TikTok-specific negotiation guidance and example emails"
      contains: "TikTok"
    - path: "knowledge_base/youtube.md"
      provides: "YouTube-specific negotiation guidance and example emails"
      contains: "YouTube"
    - path: "src/negotiation/llm/models.py"
      provides: "IntentClassification, NegotiationIntent, ProposedDeliverable, ComposedEmail, ValidationFailure, ValidationResult, EscalationPayload Pydantic models"
      exports: ["IntentClassification", "NegotiationIntent", "ValidationResult", "EscalationPayload"]
    - path: "src/negotiation/llm/client.py"
      provides: "get_anthropic_client factory and model constants"
      exports: ["get_anthropic_client", "INTENT_MODEL", "COMPOSE_MODEL"]
    - path: "src/negotiation/llm/knowledge_base.py"
      provides: "load_knowledge_base function"
      exports: ["load_knowledge_base"]
    - path: "tests/llm/test_knowledge_base.py"
      provides: "Tests for KB loading, missing file handling, platform-specific loading"
      min_lines: 30
  key_links:
    - from: "src/negotiation/llm/knowledge_base.py"
      to: "knowledge_base/"
      via: "pathlib Path resolution to project root"
      pattern: "Path.*knowledge_base"
    - from: "src/negotiation/llm/models.py"
      to: "src/negotiation/domain/types.py"
      via: "import DeliverableType for ProposedDeliverable"
      pattern: "from negotiation.domain"
---

<objective>
Install the Anthropic SDK, create all Pydantic models for LLM input/output, write the knowledge base Markdown files with per-platform negotiation guidance, and build the knowledge base loader that combines general + platform content for system prompt injection.

Purpose: Establishes the foundation that all subsequent LLM plans depend on -- models define contracts, KB provides negotiation intelligence, and the client wrapper standardizes API access.
Output: Importable `negotiation.llm` package with client, models, prompts, and knowledge_base modules; four Markdown knowledge base files; tests for KB loading.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-negotiation-pipeline/03-RESEARCH.md
@.planning/phases/01-core-domain-and-pricing-engine/01-01-SUMMARY.md
@.planning/phases/01-core-domain-and-pricing-engine/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Anthropic SDK, create LLM package with client, models, and prompts</name>
  <files>
    pyproject.toml
    uv.lock
    src/negotiation/llm/__init__.py
    src/negotiation/llm/client.py
    src/negotiation/llm/models.py
    src/negotiation/llm/prompts.py
  </files>
  <action>
    1. Install dependencies:
       - `uv add anthropic` (runtime dependency, >=0.81.0)
       - `uv add --group dev pytest-mock` (dev dependency, >=3.14)

    2. Create `src/negotiation/llm/__init__.py` with re-exports of key types from models.py and client.py. Sort __all__ alphabetically per ruff RUF022.

    3. Create `src/negotiation/llm/client.py`:
       - `get_anthropic_client() -> Anthropic` factory function. The Anthropic() constructor reads ANTHROPIC_API_KEY from environment automatically.
       - Model constants: `INTENT_MODEL = "claude-haiku-4-5-20250929"` (fast + cheap for classification), `COMPOSE_MODEL = "claude-sonnet-4-5-20250929"` (nuanced for email writing).
       - Configuration constants: `DEFAULT_CONFIDENCE_THRESHOLD = 0.70`, `DEFAULT_MAX_ROUNDS = 5`.

    4. Create `src/negotiation/llm/models.py` with ALL Pydantic models for LLM I/O:
       - `NegotiationIntent(StrEnum)`: ACCEPT, COUNTER, REJECT, QUESTION, UNCLEAR
       - `ProposedDeliverable(BaseModel)`: deliverable_type (str), quantity (int, default=1)
       - `IntentClassification(BaseModel)`: intent, confidence (float, ge=0.0, le=1.0), proposed_rate (str | None), proposed_deliverables (list[ProposedDeliverable]), summary (str), key_concerns (list[str]). Include Field descriptions for all fields as shown in research.
       - `ComposedEmail(BaseModel)`: email_body (str), model_used (str), input_tokens (int), output_tokens (int)
       - `ValidationFailure(BaseModel)`: check (str), reason (str), severity (str, default="error")
       - `ValidationResult(BaseModel)`: passed (bool), failures (list[ValidationFailure]), email_body (str)
       - `EscalationPayload(BaseModel)`: reason (str), email_draft (str), validation_failures (list[ValidationFailure]), influencer_name (str), thread_id (str), proposed_rate (Decimal | None, default=None), our_rate (Decimal | None, default=None)

    5. Create `src/negotiation/llm/prompts.py` with system prompt templates as string constants:
       - `INTENT_CLASSIFICATION_SYSTEM_PROMPT`: Template with `{negotiation_context}` placeholder. Instructions for extracting intent, rates, deliverables. Rules about proposed_rate format, confidence scoring, UNCLEAR for ambiguous emails.
       - `EMAIL_COMPOSITION_SYSTEM_PROMPT`: Template with `{knowledge_base_content}` placeholder. Rules: write ONLY email body, no subject/signature, use EXACT rate from OUR_RATE, use EXACT deliverable terms, no promises not listed, no other influencer references, 3-5 paragraphs max, address by first name. Per user decision: professional but warm tone.
       - `EMAIL_COMPOSITION_USER_PROMPT`: Template with placeholders for influencer_name, platform, negotiation_stage, their_rate, our_rate, deliverables_summary, negotiation_history.

    6. Verify:
       - `uv run ruff check src/negotiation/llm/`
       - `uv run ruff format --check src/negotiation/llm/`
       - `uv run mypy src/negotiation/llm/`
       - `uv run python -c "from negotiation.llm import get_anthropic_client, IntentClassification, NegotiationIntent, ValidationResult, EscalationPayload"`
  </action>
  <verify>
    `uv run ruff check src/negotiation/llm/ && uv run mypy src/negotiation/llm/ && uv run python -c "from negotiation.llm import get_anthropic_client, IntentClassification, NegotiationIntent"` all pass
  </verify>
  <done>
    LLM package exists with client factory, 7 Pydantic models, 3 prompt templates, and 2 model constants. All imports work, mypy strict passes, ruff clean.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create knowledge base Markdown files and loader with tests</name>
  <files>
    knowledge_base/general.md
    knowledge_base/instagram.md
    knowledge_base/tiktok.md
    knowledge_base/youtube.md
    src/negotiation/llm/knowledge_base.py
    tests/llm/__init__.py
    tests/llm/test_knowledge_base.py
  </files>
  <action>
    1. Create `knowledge_base/general.md` with cross-platform negotiation playbook:
       - "# Negotiation Playbook" heading
       - "## Core Principles" section: professional but warm tone (per user decision: "like a talent manager who's done this 1000 times"), never adversarial, acknowledge creator value before rates
       - "## Tone by Negotiation Stage" section with subsections for Initial Offer (enthusiastic, complimentary), Counter/Ours (empathetic, rational -- include rationale when countering down per user decision, skip when close to agreement), Near Agreement (encouraging, collaborative), Final Offer (direct, respectful, clear)
       - "## Do NOT Say" section: never promise exclusivity/usage rights/future deals, never reference other influencer rates, never use pressure tactics, never make unauthorized commitments
       - "## Deliverable Terminology" section: correct terms per platform (Instagram: post/story/reel; TikTok: video/story; YouTube: dedicated video/integration/short)
       - Per user decision: no signature block instruction

    2. Create `knowledge_base/instagram.md`:
       - "# Instagram Negotiation Guide"
       - "## Tone & Style": Instagram creator norms (visual-first platform, creator economy established, professional yet casual)
       - "## Negotiation Tactics" with subsections: Initial Offer, Counter-Offer Response, Final Offer
       - "## Rate Justification Templates": templates with {views}, {engagement_rate} placeholders
       - "## Example Emails" with 3 examples: Initial Offer for Reel, Counter to High Rate, Close to Agreement

    3. Create `knowledge_base/tiktok.md`:
       - Same structure as Instagram but with TikTok-specific norms (faster-paced, trend-driven, younger creators, shorter content cycles)
       - TikTok-specific terminology and example emails

    4. Create `knowledge_base/youtube.md`:
       - Same structure but YouTube-specific norms (longer-form, production value, dedicated vs integration pricing, shorts vs full videos)
       - YouTube-specific terminology and example emails

    5. Create `src/negotiation/llm/knowledge_base.py`:
       - `DEFAULT_KB_DIR = Path(__file__).resolve().parents[3] / "knowledge_base"` -- resolves from src/negotiation/llm/ up 3 levels to project root
       - `load_knowledge_base(platform: str, kb_dir: Path = DEFAULT_KB_DIR) -> str` function:
         - Loads `general.md` from kb_dir
         - Loads `{platform}.md` from kb_dir
         - Concatenates with `"\n\n---\n\n"` separator
         - Raises FileNotFoundError if neither file exists
         - Returns combined markdown string ready for system prompt injection
       - `list_available_platforms(kb_dir: Path = DEFAULT_KB_DIR) -> list[str]` utility:
         - Scans kb_dir for .md files, excludes "general.md"
         - Returns sorted list of platform names (stem of each file)

    6. Create `tests/llm/__init__.py` (empty).

    7. Create `tests/llm/test_knowledge_base.py` with tests:
       - Test load_knowledge_base returns combined content for each platform
       - Test load_knowledge_base includes general.md content
       - Test load_knowledge_base includes platform-specific content
       - Test load_knowledge_base raises FileNotFoundError for unknown platform with empty kb_dir
       - Test load_knowledge_base works with only general.md (no platform file) -- should return general only
       - Test list_available_platforms returns ["instagram", "tiktok", "youtube"]
       - Use tmp_path fixture to create test KB directories for isolation

    8. Verify:
       - `uv run pytest tests/llm/test_knowledge_base.py -v`
       - `uv run ruff check src/negotiation/llm/knowledge_base.py tests/llm/`
       - `uv run mypy src/negotiation/llm/knowledge_base.py`
  </action>
  <verify>
    `uv run pytest tests/llm/test_knowledge_base.py -v` -- all tests pass.
    `uv run ruff check src/negotiation/llm/ tests/llm/ && uv run mypy src/negotiation/llm/` -- clean.
    `ls knowledge_base/` shows general.md, instagram.md, tiktok.md, youtube.md.
  </verify>
  <done>
    Four knowledge base Markdown files exist with platform-specific negotiation guidance, example emails, and a cross-platform playbook. The loader combines general + platform content. Non-technical editors can update any .md file without code changes (KB-03). All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/llm/ -v` -- all KB loader tests pass
2. `uv run ruff check src/negotiation/llm/ tests/llm/` -- no lint errors
3. `uv run mypy src/negotiation/llm/` -- strict mode passes
4. `uv run python -c "from negotiation.llm import get_anthropic_client, IntentClassification, NegotiationIntent, EscalationPayload, ValidationResult"` -- all imports work
5. `ls knowledge_base/` -- shows general.md, instagram.md, tiktok.md, youtube.md
6. Knowledge base files contain: negotiation tactics, tone rules, example emails, and per-platform sections (per user decisions)
7. Knowledge base files are in top-level directory outside src/ (editable without code changes per KB-03)
</verification>

<success_criteria>
- Anthropic SDK (>=0.81.0) and pytest-mock installed and importable
- 7 Pydantic models define all LLM I/O contracts (IntentClassification, ComposedEmail, ValidationFailure, ValidationResult, EscalationPayload, ProposedDeliverable, NegotiationIntent)
- Client factory returns configured Anthropic client with INTENT_MODEL and COMPOSE_MODEL constants
- System prompt templates are parameterized and ready for knowledge base injection
- 4 knowledge base Markdown files with negotiation guidance, example emails, per-platform sections
- Knowledge base loader returns combined content for any supported platform
- All tests pass, mypy strict clean, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-negotiation-pipeline/03-01-SUMMARY.md`
</output>
