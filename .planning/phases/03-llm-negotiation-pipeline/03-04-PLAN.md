---
phase: 03-llm-negotiation-pipeline
plan: 04
type: execute
wave: 3
depends_on:
  - 03-02
  - 03-03
files_modified:
  - src/negotiation/llm/negotiation_loop.py
  - src/negotiation/llm/__init__.py
  - tests/llm/test_negotiation_loop.py
autonomous: true
requirements:
  - NEG-05
  - NEG-06

must_haves:
  truths:
    - "The end-to-end negotiation loop works: email arrives -> intent is classified -> pricing engine calculates response rate -> LLM composes email -> validation gate checks monetary values -> email is sent or escalated"
    - "When max autonomous rounds are reached, the loop escalates to human rather than continuing"
    - "When intent confidence is low, the loop escalates to human rather than guessing"
    - "When the influencer accepts, the loop transitions to AGREED state and returns accept action"
    - "When the influencer rejects, the loop transitions to REJECTED state and returns reject action"
    - "When validation fails, the loop produces an EscalationPayload with draft and failure reasons instead of sending"
    - "When the proposed rate exceeds CPM ceiling, the loop escalates with pricing context"
  artifacts:
    - path: "src/negotiation/llm/negotiation_loop.py"
      provides: "process_influencer_reply orchestrator function"
      exports: ["process_influencer_reply"]
      min_lines: 60
    - path: "tests/llm/test_negotiation_loop.py"
      provides: "Integration tests for the full negotiation loop"
      min_lines: 80
    - path: "src/negotiation/llm/__init__.py"
      provides: "Updated re-exports including all public API functions"
      exports: ["process_influencer_reply", "classify_intent", "compose_counter_email", "validate_composed_email", "load_knowledge_base"]
  key_links:
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/llm/intent.py"
      via: "imports classify_intent"
      pattern: "from negotiation.llm.intent import"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/llm/composer.py"
      via: "imports compose_counter_email"
      pattern: "from negotiation.llm.composer import"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/llm/validation.py"
      via: "imports validate_composed_email"
      pattern: "from negotiation.llm.validation import"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/pricing/boundaries.py"
      via: "imports evaluate_proposed_rate from Phase 1 pricing engine"
      pattern: "from negotiation.pricing import"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/pricing/engine.py"
      via: "imports calculate_rate for computing our counter-offer rate"
      pattern: "from negotiation.pricing import"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/state_machine/machine.py"
      via: "uses NegotiationStateMachine.trigger() for state transitions"
      pattern: "state_machine\\.trigger"
    - from: "src/negotiation/llm/negotiation_loop.py"
      to: "src/negotiation/llm/knowledge_base.py"
      via: "imports load_knowledge_base for KB content injection"
      pattern: "from negotiation.llm.knowledge_base import"
---

<objective>
Build the end-to-end negotiation loop orchestrator that wires together intent classification, the pricing engine, email composition, the validation gate, and the state machine into a single `process_influencer_reply` function.

Purpose: This is the critical integration point -- the "brain" that decides what action to take for each influencer reply by coordinating all Phase 1-3 components.
Output: A fully tested `process_influencer_reply` function that handles every branch: accept, reject, counter, question, escalation (low confidence, max rounds, CPM exceeded, validation failure).
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-negotiation-pipeline/03-RESEARCH.md
@.planning/phases/03-llm-negotiation-pipeline/03-01-SUMMARY.md
@.planning/phases/03-llm-negotiation-pipeline/03-02-SUMMARY.md
@.planning/phases/03-llm-negotiation-pipeline/03-03-SUMMARY.md
@src/negotiation/llm/intent.py
@src/negotiation/llm/composer.py
@src/negotiation/llm/validation.py
@src/negotiation/llm/knowledge_base.py
@src/negotiation/llm/models.py
@src/negotiation/pricing/__init__.py
@src/negotiation/state_machine/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement process_influencer_reply orchestrator</name>
  <files>
    src/negotiation/llm/negotiation_loop.py
    src/negotiation/llm/__init__.py
  </files>
  <action>
    1. Create `src/negotiation/llm/negotiation_loop.py` with `process_influencer_reply` function:

       Parameters:
       - email_body: str -- the influencer's reply text
       - negotiation_context: dict -- must contain keys: influencer_name (str), thread_id (str), platform (str), average_views (int), deliverables_summary (str), deliverable_types (list[str]), next_cpm (Decimal), history (str, optional)
       - state_machine: NegotiationStateMachine -- current negotiation state machine instance
       - client: Anthropic -- configured Anthropic client
       - round_count: int -- current round number (0-based)
       - max_rounds: int = DEFAULT_MAX_ROUNDS (5)

       Returns: dict with "action" key (one of: "send", "escalate", "accept", "reject")

       Logic flow (in order):

       Step 1 - Check round cap:
       If round_count >= max_rounds, return {"action": "escalate", "reason": "Max autonomous rounds ({max_rounds}) reached", "payload": EscalationPayload(...)}

       Step 2 - Load knowledge base:
       kb_content = load_knowledge_base(negotiation_context["platform"])

       Step 3 - Classify intent:
       classification = classify_intent(email_body, str(negotiation_context), client)

       Step 4 - Handle UNCLEAR intent:
       If classification.intent == UNCLEAR, return {"action": "escalate", "reason": f"Low confidence intent: {classification.confidence}", "classification": classification}

       Step 5 - Handle ACCEPT:
       If classification.intent == ACCEPT, trigger state_machine.trigger("accept"), return {"action": "accept", "classification": classification}

       Step 6 - Handle REJECT:
       If classification.intent == REJECT, trigger state_machine.trigger("reject"), return {"action": "reject", "classification": classification}

       Step 7 - Handle COUNTER or QUESTION -- calculate pricing:
       Trigger state_machine.trigger("receive_reply")

       If classification.proposed_rate is not None:
         proposed = Decimal(classification.proposed_rate)
         pricing = evaluate_proposed_rate(proposed_rate=proposed, average_views=negotiation_context["average_views"])
         If pricing.should_escalate:
           state_machine.trigger("escalate")
           return {"action": "escalate", "reason": pricing.warning, "pricing": pricing, "classification": classification}

       Step 8 - Calculate our counter rate:
       our_rate = calculate_rate(negotiation_context["average_views"], negotiation_context["next_cpm"])

       Step 9 - Compose counter-offer email:
       composed = compose_counter_email(
         influencer_name=negotiation_context["influencer_name"],
         their_rate=classification.proposed_rate or "not specified",
         our_rate=str(our_rate),
         deliverables_summary=negotiation_context["deliverables_summary"],
         platform=negotiation_context["platform"],
         negotiation_stage="counter" if classification.intent == COUNTER else "question_response",
         knowledge_base_content=kb_content,
         negotiation_history=negotiation_context.get("history", ""),
         client=client,
       )

       Step 10 - Validate before sending:
       validation = validate_composed_email(
         email_body=composed.email_body,
         expected_rate=our_rate,
         expected_deliverables=negotiation_context["deliverable_types"],
         influencer_name=negotiation_context["influencer_name"],
       )

       If not validation.passed:
         return {"action": "escalate", "reason": "Validation failed", "payload": EscalationPayload(reason="Email validation failed", email_draft=composed.email_body, validation_failures=validation.failures, influencer_name=negotiation_context["influencer_name"], thread_id=negotiation_context["thread_id"], our_rate=our_rate)}

       Step 11 - Email validated, trigger send:
       state_machine.trigger("send_counter")
       return {"action": "send", "email_body": composed.email_body, "our_rate": our_rate, "round": round_count + 1, "classification": classification}

    2. Update `src/negotiation/llm/__init__.py` to re-export all public API:
       - From intent: classify_intent
       - From composer: compose_counter_email
       - From validation: validate_composed_email
       - From knowledge_base: load_knowledge_base, list_available_platforms
       - From negotiation_loop: process_influencer_reply
       - From models: all model classes
       - From client: get_anthropic_client, INTENT_MODEL, COMPOSE_MODEL, DEFAULT_CONFIDENCE_THRESHOLD, DEFAULT_MAX_ROUNDS
       - Sort __all__ alphabetically per ruff RUF022

    3. Verify:
       - `uv run ruff check src/negotiation/llm/`
       - `uv run mypy src/negotiation/llm/`
       - `uv run python -c "from negotiation.llm import process_influencer_reply, classify_intent, compose_counter_email, validate_composed_email, load_knowledge_base"`
  </action>
  <verify>
    `uv run ruff check src/negotiation/llm/ && uv run mypy src/negotiation/llm/ && uv run python -c "from negotiation.llm import process_influencer_reply"` all pass
  </verify>
  <done>
    process_influencer_reply orchestrator exists with all 11 steps implemented. __init__.py re-exports all public API. All imports work, mypy strict clean, ruff clean.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integration tests for the negotiation loop</name>
  <files>
    tests/llm/test_negotiation_loop.py
  </files>
  <action>
    1. Create `tests/llm/test_negotiation_loop.py` with integration tests that mock the LLM calls but use real pricing engine and state machine:

       Fixtures:
       - mock_client: MagicMock Anthropic client
       - base_context: dict with standard negotiation context (influencer_name="Jane Creator", platform="instagram", average_views=100000, deliverables_summary="1 Instagram Reel", deliverable_types=["instagram_reel"], thread_id="thread_123", next_cpm=Decimal("25"))
       - state_machine: NegotiationStateMachine started at AWAITING_REPLY state (trigger "send_offer" first to advance from INITIAL_OFFER)
       - Helper to configure mock client for both classify (messages.parse) and compose (messages.create) responses

    2. Test cases:

       a. test_max_rounds_escalation:
          - round_count=5, max_rounds=5
          - Result: action="escalate", reason contains "Max autonomous rounds"
          - State machine unchanged (no trigger called)

       b. test_unclear_intent_escalation:
          - Mock classify returns UNCLEAR with confidence=0.3
          - Result: action="escalate", reason contains "Low confidence"

       c. test_accept_transitions_to_agreed:
          - Mock classify returns ACCEPT with confidence=0.95
          - Result: action="accept"
          - State machine state is AGREED (terminal)

       d. test_reject_transitions_to_rejected:
          - Mock classify returns REJECT with confidence=0.90
          - Result: action="reject"
          - State machine state is REJECTED (terminal)

       e. test_counter_within_range_sends_email:
          - Mock classify returns COUNTER with proposed_rate="1500.00", confidence=0.92
          - Mock compose returns email body containing "$2,500.00" (matching our_rate)
          - Result: action="send", email_body present, our_rate present
          - State machine state is COUNTER_SENT

       f. test_counter_exceeds_ceiling_escalates:
          - Mock classify returns COUNTER with proposed_rate="50000.00" (ridiculously high CPM)
          - Result: action="escalate", reason contains CPM warning
          - State machine state is ESCALATED

       g. test_validation_failure_escalates:
          - Mock classify returns COUNTER with proposed_rate="1500.00"
          - Mock compose returns email body containing "$999.00" (wrong amount -- will fail validation)
          - Result: action="escalate", reason="Validation failed", payload contains validation_failures

       h. test_question_intent_composes_response:
          - Mock classify returns QUESTION with confidence=0.85, no proposed_rate
          - Mock compose returns email body with correct rate
          - Result: action="send" (questions get informational responses)
          - negotiation_stage in compose call is "question_response"

       i. test_low_confidence_counter_overridden_to_unclear:
          - Mock classify returns COUNTER with confidence=0.4 (below 0.70 threshold)
          - classify_intent should override to UNCLEAR
          - Result: action="escalate"

    3. Verify:
       - `uv run pytest tests/llm/test_negotiation_loop.py -v`
       - `uv run pytest tests/llm/ -v` (all LLM tests together)
       - `uv run pytest tests/ -v` (full test suite -- ensure no regressions)
       - `uv run ruff check tests/llm/`
       - `uv run mypy src/negotiation/llm/`
  </action>
  <verify>
    `uv run pytest tests/llm/ -v` -- all tests pass.
    `uv run pytest tests/ -v` -- full suite passes (no regressions in Phase 1/2 tests).
    `uv run ruff check src/negotiation/llm/ tests/llm/ && uv run mypy src/negotiation/llm/` -- clean.
  </verify>
  <done>
    9+ integration tests verify all branches of the negotiation loop: accept, reject, counter (send), counter (escalate for CPM), question, unclear, max rounds, validation failure. Full test suite passes with no regressions. The end-to-end flow works: email -> classify -> price -> compose -> validate -> send/escalate.
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/ -v` -- ALL tests pass (Phase 1 + Phase 2 + Phase 3)
2. `uv run ruff check src/ tests/` -- no lint errors anywhere
3. `uv run mypy src/` -- strict mode passes for all source
4. `uv run python -c "from negotiation.llm import process_influencer_reply, classify_intent, compose_counter_email, validate_composed_email, load_knowledge_base, get_anthropic_client"` -- all public API importable
5. The negotiation loop correctly handles every branch: accept -> AGREED, reject -> REJECTED, counter within range -> send, counter above ceiling -> escalate, low confidence -> escalate, max rounds -> escalate, validation failure -> escalate with payload
</verification>

<success_criteria>
- process_influencer_reply orchestrates the full negotiation loop with clear action-based returns
- All 7 escalation triggers work: max rounds, low confidence, CPM exceeded, validation failure, and their variations
- State machine transitions are correct for each action (accept -> AGREED, reject -> REJECTED, counter -> COUNTER_SENT, escalation -> ESCALATED)
- Integration tests use real pricing engine and state machine (only LLM calls are mocked)
- Full test suite (all phases) passes with no regressions
- mypy strict clean, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-negotiation-pipeline/03-04-SUMMARY.md`
</output>
