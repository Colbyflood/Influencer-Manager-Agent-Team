---
phase: 04-slack-and-human-in-the-loop
plan: 02
type: tdd
wave: 2
depends_on:
  - 04-01
files_modified:
  - src/negotiation/slack/triggers.py
  - src/negotiation/slack/__init__.py
  - tests/slack/test_triggers.py
autonomous: true
requirements:
  - HUMAN-02

must_haves:
  truths:
    - "Trigger config loads from YAML file and validates via Pydantic with all 5 trigger types"
    - "Missing or empty YAML file falls back to all-defaults (all triggers enabled, CPM threshold 30.0)"
    - "CPM-over-threshold trigger fires when proposed CPM exceeds configured threshold"
    - "Ambiguous intent trigger fires when intent classification confidence is below threshold"
    - "LLM-based triggers detect hostile tone, legal language, and unusual deliverables with quoted evidence"
    - "Disabled triggers do not fire even when conditions are met"
    - "evaluate_triggers returns a list of TriggerResults with specific reasons and evidence quotes"
  artifacts:
    - path: "src/negotiation/slack/triggers.py"
      provides: "TriggerType enum, config models, load_triggers_config, classify_triggers, evaluate_triggers"
      min_lines: 120
      exports: ["TriggerType", "TriggerConfig", "EscalationTriggersConfig", "TriggerResult", "load_triggers_config", "classify_triggers", "evaluate_triggers"]
    - path: "tests/slack/test_triggers.py"
      provides: "Tests for config loading, deterministic triggers, LLM classification, full evaluation"
      min_lines: 150
  key_links:
    - from: "src/negotiation/slack/triggers.py"
      to: "config/escalation_triggers.yaml"
      via: "yaml.safe_load reads config file"
      pattern: "yaml\\.safe_load"
    - from: "src/negotiation/slack/triggers.py"
      to: "anthropic.Anthropic"
      via: "client.messages.parse for LLM trigger classification"
      pattern: "messages\\.parse"
---

<objective>
Build the escalation trigger engine that evaluates configurable rules against influencer emails -- combining deterministic triggers (CPM threshold, ambiguous intent) with LLM-based triggers (hostile tone, legal language, unusual deliverables).

Purpose: HUMAN-02 requires configurable trigger rules. The trigger engine runs as a pre-processing gate before the negotiation loop, catching emails that should be escalated before any autonomous response.
Output: Tested trigger engine with YAML config loading, deterministic evaluation, and LLM classification.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/04-slack-and-human-in-the-loop/04-RESEARCH.md
@.planning/phases/04-slack-and-human-in-the-loop/04-CONTEXT.md
@.planning/phases/04-slack-and-human-in-the-loop/04-01-SUMMARY.md
@config/escalation_triggers.yaml
@src/negotiation/llm/client.py
</context>

<feature>
  <name>Escalation Trigger Engine</name>
  <files>src/negotiation/slack/triggers.py, tests/slack/test_triggers.py</files>
  <behavior>
    The trigger engine evaluates 5 escalation triggers against an influencer email:

    **Deterministic triggers:**
    1. `cpm_over_threshold`: Given proposed_rate and average_views, if calculated CPM > config threshold -> fires with reason "CPM ${cpm} exceeds threshold ${threshold}"
    2. `ambiguous_intent`: Given intent classification confidence < 0.70 (or configured) -> fires with reason "Intent confidence {conf} below threshold"

    **LLM-based triggers (single Haiku call):**
    3. `hostile_tone`: Email contains aggressive, threatening, or hostile language -> fires with quoted evidence
    4. `legal_language`: Email contains legal/contract references -> fires with quoted evidence
    5. `unusual_deliverables`: Email requests non-standard deliverables -> fires with quoted evidence

    **Config loading:**
    - `load_triggers_config(path)` -> `EscalationTriggersConfig`
    - Missing file -> all defaults (all enabled, CPM threshold 30.0)
    - Invalid YAML -> all defaults (log warning but don't crash)

    **LLM classification:**
    - `classify_triggers(email_body, client, model)` -> `TriggerClassification`
    - Single API call classifies all 3 LLM triggers simultaneously
    - Returns bool + evidence string for each trigger
    - Uses Claude Haiku (INTENT_MODEL) for speed/cost per locked decision

    **Full evaluation:**
    - `evaluate_triggers(email_body, proposed_cpm, intent_confidence, config, client)` -> `list[TriggerResult]`
    - Checks each enabled trigger in order
    - Returns only fired triggers (empty list = no escalation needed)
    - Skips LLM call entirely if all 3 LLM triggers are disabled

    Cases:
    - email with CPM 35.0, threshold 30.0 -> [TriggerResult(cpm_over_threshold, fired=True, reason="CPM $35.00 exceeds threshold $30.00")]
    - email with confidence 0.5 -> [TriggerResult(ambiguous_intent, fired=True, reason="Intent confidence 0.50 below threshold")]
    - hostile email "I'll make sure no one works with you again" -> [TriggerResult(hostile_tone, fired=True, evidence="I'll make sure no one works with you again")]
    - legal email "my lawyer will review the contract terms" -> [TriggerResult(legal_language, fired=True, evidence="my lawyer will review the contract terms")]
    - benign email, all conditions normal -> [] (empty list)
    - cpm_over_threshold disabled, CPM 35.0 -> [] (trigger doesn't fire)
  </behavior>
  <implementation>
    1. Create `TriggerType` StrEnum with 5 values per RESEARCH.md.
    2. Create `TriggerConfig` Pydantic model with `enabled: bool = True` and optional `cpm_threshold: float | None = None` and `always_trigger_keywords: list[str]`.
    3. Create `EscalationTriggersConfig` Pydantic model with 5 TriggerConfig fields, all defaulting to enabled.
    4. Create `TriggerResult` Pydantic model with `trigger_type`, `fired`, `reason`, `evidence`.
    5. Create `TriggerClassification` Pydantic model with bool + evidence for each LLM trigger (used as structured output schema).
    6. Implement `load_triggers_config(path)` using `yaml.safe_load` + Pydantic `model_validate`.
    7. Implement `classify_triggers(email_body, client, model)` using `client.messages.parse` with TriggerClassification output schema and the system prompt from RESEARCH.md.
    8. Implement `evaluate_triggers(email_body, proposed_cpm, intent_confidence, config, client)`:
       - Check deterministic triggers first (no API cost)
       - Only call `classify_triggers` if any LLM trigger is enabled
       - Return list of fired TriggerResults
    9. Use `email.utils.parseaddr` for any email parsing (per RESEARCH.md "don't hand-roll" guidance).
    10. Update `src/negotiation/slack/__init__.py` to export trigger-related symbols.
  </implementation>
</feature>

<verification>
1. `uv run pytest tests/slack/test_triggers.py -v` -- all trigger tests pass
2. `uv run pytest` -- full suite passes
3. `uv run ruff check src/negotiation/slack/triggers.py tests/slack/test_triggers.py` -- no lint errors
4. `uv run mypy src/negotiation/slack/` -- no type errors
</verification>

<success_criteria>
- YAML config loads correctly with Pydantic validation
- Missing/empty config falls back to safe defaults
- CPM threshold trigger fires correctly at boundary
- Ambiguous intent trigger fires on low confidence
- LLM classification detects hostile tone, legal language, unusual deliverables with evidence
- Disabled triggers are respected
- evaluate_triggers returns only fired triggers
- All tests pass including full regression suite
</success_criteria>

<output>
After completion, create `.planning/phases/04-slack-and-human-in-the-loop/04-02-SUMMARY.md`
</output>
