---
phase: 03-llm-negotiation-pipeline
plan: 02
type: tdd
wave: 2
depends_on:
  - 03-01
files_modified:
  - src/negotiation/llm/intent.py
  - tests/llm/test_intent.py
autonomous: true
requirements:
  - NEG-05

must_haves:
  truths:
    - "Given a free-text influencer email reply, the agent correctly extracts the negotiation intent (accept, counter, reject, question, unclear)"
    - "Rate proposals are extracted as string dollar amounts from influencer emails"
    - "Deliverable changes are extracted when the influencer proposes different deliverables"
    - "Low-confidence classifications are overridden to UNCLEAR for human escalation"
    - "The confidence threshold is configurable (default 0.70)"
  artifacts:
    - path: "src/negotiation/llm/intent.py"
      provides: "classify_intent function using Claude structured outputs"
      exports: ["classify_intent"]
      min_lines: 30
    - path: "tests/llm/test_intent.py"
      provides: "Comprehensive tests for intent classification with mocked API"
      min_lines: 60
  key_links:
    - from: "src/negotiation/llm/intent.py"
      to: "src/negotiation/llm/models.py"
      via: "imports IntentClassification, NegotiationIntent"
      pattern: "from negotiation.llm.models import"
    - from: "src/negotiation/llm/intent.py"
      to: "src/negotiation/llm/prompts.py"
      via: "imports INTENT_CLASSIFICATION_SYSTEM_PROMPT"
      pattern: "from negotiation.llm.prompts import"
    - from: "src/negotiation/llm/intent.py"
      to: "anthropic.Anthropic"
      via: "client.messages.parse() with output_format=IntentClassification"
      pattern: "client\\.messages\\.parse"
---

<objective>
Build the intent classification module that extracts negotiation intent, rate proposals, and deliverable changes from free-text influencer email replies using Claude's structured outputs.

Purpose: This is the "understanding" half of the LLM pipeline -- the agent must correctly interpret what an influencer is saying before it can respond.
Output: Tested `classify_intent` function that returns a validated `IntentClassification` Pydantic model from any influencer email.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-llm-negotiation-pipeline/03-RESEARCH.md
@.planning/phases/03-llm-negotiation-pipeline/03-01-SUMMARY.md
@src/negotiation/llm/models.py
@src/negotiation/llm/prompts.py
@src/negotiation/llm/client.py
</context>

<feature>
  <name>Intent Classification with Structured Outputs</name>
  <files>src/negotiation/llm/intent.py, tests/llm/test_intent.py</files>
  <behavior>
    classify_intent(email_body, negotiation_context, client, model?, confidence_threshold?) -> IntentClassification

    Cases:
    - Clear acceptance email ("Sounds great, let's do it!") -> intent=ACCEPT, confidence>=0.8
    - Counter-offer with rate ("I'd love to but I charge $2,000") -> intent=COUNTER, proposed_rate="2000.00", confidence>=0.8
    - Counter with deliverable change ("Could we do 2 reels instead?") -> intent=COUNTER, proposed_deliverables populated
    - Rejection ("I'll pass on this one") -> intent=REJECT, confidence>=0.8
    - Question ("What exactly does the partnership include?") -> intent=QUESTION, key_concerns populated
    - Ambiguous with low confidence (model returns confidence=0.4 for ACCEPT) -> intent overridden to UNCLEAR
    - Intent already UNCLEAR (model returns UNCLEAR) -> no override, stays UNCLEAR
    - Confidence at exactly threshold (0.70) -> NOT overridden (threshold is exclusive: < threshold triggers override)

    All tests use mocked Anthropic client -- no real API calls.
  </behavior>
  <implementation>
    1. classify_intent function:
       - Accepts email_body (str), negotiation_context (str), client (Anthropic), model (str, default=INTENT_MODEL), confidence_threshold (float, default=DEFAULT_CONFIDENCE_THRESHOLD)
       - Calls client.messages.parse() with output_format=IntentClassification
       - Uses INTENT_CLASSIFICATION_SYSTEM_PROMPT.format(negotiation_context=negotiation_context) as system prompt
       - User message: "Classify the intent of this influencer email reply:\n\n{email_body}"
       - After receiving result, applies confidence threshold override: if confidence < threshold AND intent != UNCLEAR, override intent to UNCLEAR via model_copy(update={"intent": NegotiationIntent.UNCLEAR})
       - Returns the IntentClassification

    2. Testing approach:
       - Create mock_anthropic_client fixture returning MagicMock
       - Create make_mock_parse_response helper that wraps IntentClassification in mock response with .parsed_output
       - Test each intent type (ACCEPT, COUNTER, REJECT, QUESTION, UNCLEAR)
       - Test confidence threshold override behavior
       - Test that client.messages.parse is called with correct arguments (model, output_format, system, messages)
       - Test proposed_rate extraction for counter-offers
       - Test proposed_deliverables population
       - Test key_concerns population for questions
  </implementation>
</feature>

<verification>
1. `uv run pytest tests/llm/test_intent.py -v` -- all tests pass (RED must fail first, GREEN must pass)
2. `uv run ruff check src/negotiation/llm/intent.py tests/llm/test_intent.py` -- no lint errors
3. `uv run mypy src/negotiation/llm/intent.py` -- strict mode passes
</verification>

<success_criteria>
- classify_intent function correctly delegates to Claude structured outputs and returns IntentClassification
- Low-confidence results are overridden to UNCLEAR (configurable threshold, default 0.70)
- All 8+ test cases pass with mocked API client
- No real API calls made during testing
- mypy strict clean, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/03-llm-negotiation-pipeline/03-02-SUMMARY.md`
</output>
